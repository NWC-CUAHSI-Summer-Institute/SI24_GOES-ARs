{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a786a244-1419-482f-806e-38352d8650a6",
   "metadata": {},
   "source": [
    "# GOES Historical Data Access from GridSat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e6596b-2a95-49d0-b16b-019172055df6",
   "metadata": {},
   "source": [
    "This code will help download the historical GOES data in bulk using GridSat https://www.ncei.noaa.gov/data/gridsat-goes/access/goes/, store it in csvs along with the download link. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ca28e71-b6a4-4bb2-8716-8989e731d553",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c670c8-44bc-43cf-9884-a43c79fbf044",
   "metadata": {},
   "source": [
    "First, we want to see the list of dates with labeled data. Since we initially wrote a code to store the dataset in a numpy array, we need to convert it to the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dddfb7bc-ff9e-45ac-8675-981cf9aac30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list from the .npy file \n",
    "# west_ar.npy file contains the list of labeled AR data from ClimateNet (https://gmd.copernicus.org/articles/14/107/2021/)\n",
    "file_list = np.load('west_ar.npy', allow_pickle=True)\n",
    "\n",
    "# Ensure it is a list (if it's stored as a numpy array, you might need to convert it)\n",
    "if isinstance(file_list, np.ndarray):\n",
    "    file_list = file_list.tolist()\n",
    "    \n",
    "#file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a625f0b5-438b-476f-9156-c2493df7067a",
   "metadata": {},
   "source": [
    "# GOES all with links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60538cb-c328-4ab8-ad32-5c1b0d93ce08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Base url from the GridSat\n",
    "base_url = \"https://www.ncei.noaa.gov/data/gridsat-goes/access/goes/\"\n",
    "\n",
    "# Function to fetch and parse HTML content from a URL\n",
    "def fetch_html(url):\n",
    "    response = requests.get(url)\n",
    "    return response.content if response.status_code == 200 else None\n",
    "\n",
    "# Function to parse the list of .nc files from HTML content\n",
    "def parse_nc_files(html_content, year, month):\n",
    "    data = []\n",
    "    if html_content:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Find all links ending with .nc (assuming they are the .nc files)\n",
    "        nc_links = soup.find_all('a', href=lambda href: href and href.endswith('.nc'))\n",
    "        for link in nc_links:\n",
    "            nc_file_url = urljoin(year_month_url, link['href'])\n",
    "            # Extract year, month, day, hour, and GOES number from URL\n",
    "            parts = link['href'].split('.')\n",
    "            year = int(parts[2])\n",
    "            month = int(parts[3])\n",
    "            day = int(parts[4])\n",
    "            hour = parts[5][:2]  # Extract first two characters for hour\n",
    "            goesXX = parts[1].replace('goes', '')  # Extracts goesXX from the filename\n",
    "            \n",
    "            # Determine east or west\n",
    "            if goesXX in ['08', '12', '13']:\n",
    "                goes_direction = 'east'\n",
    "            else:\n",
    "                goes_direction = 'west'\n",
    "            \n",
    "            hour_minute = f\"{hour}:00\"\n",
    "            data.append([year, month, day, hour_minute, goesXX, goes_direction, nc_file_url])\n",
    "    return data\n",
    "\n",
    "# CSV filename\n",
    "csv_filename = 'goes_all_w_links.csv'\n",
    "\n",
    "# Iterate over years from 1996 to 2017 (adjust as needed - depends on which year are you interested in exploring)\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Year', 'Month', 'Day', 'Hour', 'GOES_Num', 'Direction', 'URL'])  # Write header\n",
    "    \n",
    "    for year in range(1996, 2017):\n",
    "        for month in range(1, 13):\n",
    "            year_month_url = f\"{base_url}{year}/{month:02d}/\"\n",
    "            html_content = fetch_html(year_month_url)\n",
    "            if html_content:\n",
    "                nc_data = parse_nc_files(html_content, year, month)\n",
    "                for row in nc_data:\n",
    "                    csv_writer.writerow(row)\n",
    "                    print(f\"Saved: {row}\")\n",
    "\n",
    "#print(f\"CSV file '{csv_filename}' has been successfully created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0727bed-4ac7-430a-98cf-4d57e3f36f62",
   "metadata": {},
   "source": [
    "# GOES West only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a81cc7-ce9a-4381-987d-6624f48dcf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part only downloads goes_west data\n",
    "\n",
    "# Load the CSV file\n",
    "csv_filename = 'goes_all_w_links.csv'\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Filter out rows with 'goes_direction' containing \"08\", \"12\", or \"13\"\n",
    "df_filtered = df[~df['Direction'].isin([\"east\"])]\n",
    "\n",
    "# df_filtered.to_csv('/Users/surabhiupadhyay/Documents/aether/GOES_hist/goes_west.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1adf49-afe1-40a1-bd07-30983435ce2d",
   "metadata": {},
   "source": [
    "# Extract Data 7 days prior and 2 days after an AR landfall event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ad0ad9-b3f8-45fe-b543-321a3b6a8e14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This part helps me download \n",
    "# Load the list from the .npy file\n",
    "file_list = np.load('/Users/surabhiupadhyay/Downloads/west_ar.npy', allow_pickle=True)\n",
    "\n",
    "# Ensure it is a list (if it's stored as a numpy array, you might need to convert it)\n",
    "if isinstance(file_list, np.ndarray):\n",
    "    file_list = file_list.tolist()\n",
    "\n",
    "# Function to extract date from file path\n",
    "def extract_date_from_filepath(filepath):\n",
    "    match = re.search(r'\\d{4}-\\d{2}-\\d{2}', filepath)\n",
    "    return match.group(0) if match else None\n",
    "\n",
    "# Extract dates from files_list\n",
    "file_dates = {extract_date_from_filepath(fp) for fp in file_list if extract_date_from_filepath(fp)}\n",
    "\n",
    "# Load the CSV file\n",
    "csv_filename = r'goes_west.csv'\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Extract URLs from the CSV\n",
    "urls = df['URL'].tolist()\n",
    "\n",
    "# Function to extract date from URL\n",
    "def extract_date_from_url(url):\n",
    "    match = re.search(r'\\d{4}\\.\\d{2}\\.\\d{2}', url)\n",
    "    return match.group(0).replace('.', '-') if match else None\n",
    "\n",
    "# Extract dates from URLs\n",
    "url_dates = {extract_date_from_url(url) for url in urls if extract_date_from_url(url)}\n",
    "\n",
    "# Find common dates\n",
    "common_dates = file_dates.intersection(url_dates)\n",
    "\n",
    "# Function to get URLs for a given date\n",
    "def get_urls_for_date(date_str, url_list):\n",
    "    date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "    start_date = date_obj - timedelta(days=7)\n",
    "    end_date = date_obj + timedelta(days=3)\n",
    "    relevant_urls = []\n",
    "    \n",
    "    for url in url_list:\n",
    "        url_date_str = extract_date_from_url(url)\n",
    "        if url_date_str:\n",
    "            url_date_obj = datetime.strptime(url_date_str, '%Y-%m-%d')\n",
    "            if start_date <= url_date_obj <= end_date:\n",
    "                relevant_urls.append(url)\n",
    "                \n",
    "    return relevant_urls\n",
    "\n",
    "# Get all relevant URLs\n",
    "all_relevant_urls = []\n",
    "for date in common_dates:\n",
    "    all_relevant_urls.extend(get_urls_for_date(date, urls))\n",
    "\n",
    "# Remove duplicates\n",
    "all_relevant_urls = list(set(all_relevant_urls))\n",
    "\n",
    "# Writing relevant URLs to a new CSV file\n",
    "#output_csv_filename = 'relevant_data_with_urls.csv'\n",
    "#output_df = pd.DataFrame({'URL': all_relevant_urls})\n",
    "#output_df.to_csv(output_csv_filename, index=False)\n",
    "#print(f\"Relevant URLs saved to {output_csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d120c8c-bfa8-4e49-a5e4-20d9b2607ccc",
   "metadata": {},
   "source": [
    "# Extract 00, 06, 12, and 18 hours from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b821d-ceb4-4541-b4ff-679b44d3a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# Load the CSV file\n",
    "csv_filename = r'relevant_data_with_urls.csv'\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Assuming 'Year', 'Month', 'Day' are columns already present in your DataFrame\n",
    "# Create 'Date' column from 'Year', 'Month', 'Day'\n",
    "df['Date'] = df.apply(lambda row: f\"{int(row['Year']):04d}-{int(row['Month']):02d}-{int(row['Day']):02d}\", axis=1)\n",
    "\n",
    "# Ensure 'Hour' column is in hh:mm format\n",
    "# Example: '06:00' -> '06:00:00'\n",
    "df['Hour'] = df['Hour'].apply(lambda x: x + ':00')\n",
    "\n",
    "# Convert 'Hour' column to timedelta for comparison\n",
    "df['Hour'] = pd.to_timedelta(df['Hour'])\n",
    "\n",
    "# Define target hours as timedeltas\n",
    "target_hours = [pd.to_timedelta(h) for h in ['00:00:00', '06:00:00', '12:00:00', '18:00:00']]\n",
    "\n",
    "# Function to find the closest hour\n",
    "def find_closest_hour(hours, target_hour):\n",
    "    time_diffs = [abs(h - target_hour) for h in hours]\n",
    "    closest_hour = hours[time_diffs.index(min(time_diffs))]\n",
    "    return closest_hour\n",
    "\n",
    "# Drop duplicates based on 'Hour' column\n",
    "df_no_duplicates = df.sort_values(by='Hour').drop_duplicates(subset=['Date', 'Hour'], keep='first')\n",
    "\n",
    "# Filter and find the nearest available hour for each date and target hour\n",
    "filtered_rows = []\n",
    "for target_hour in target_hours:\n",
    "    for date in df_no_duplicates['Date'].unique():\n",
    "        daily_data = df_no_duplicates[df_no_duplicates['Date'] == date]\n",
    "        if not daily_data.empty:\n",
    "            available_hours = daily_data['Hour'].tolist()\n",
    "            closest_hour = find_closest_hour(available_hours, target_hour)\n",
    "            filtered_rows.append(daily_data[daily_data['Hour'] == closest_hour])\n",
    "\n",
    "# Concatenate filtered rows into a new DataFrame\n",
    "filtered_df = pd.concat(filtered_rows)\n",
    "\n",
    "# Sort the filtered DataFrame\n",
    "filtered_df.sort_values(by=['Date', 'Hour'], inplace=True)\n",
    "\n",
    "# Writing the filtered DataFrame to a new CSV file\n",
    "output_csv_filename = 'relevant_data_with_urls_filtered.csv'\n",
    "filtered_df.to_csv(output_csv_filename, index=False)\n",
    "print(f\"Filtered data with relevant URLs saved to {output_csv_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
